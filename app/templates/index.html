<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>MedAgent - AI Healthcare Assistant</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <style>
      body {
        background-color: #f8f9fa;
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
      }
      .chat-container {
        max-width: 700px;
        margin: 2rem auto;
        border-radius: 10px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        background: white;
        height: 75vh;
        display: flex;
        flex-direction: column;
      }
      .chat-header {
        background: linear-gradient(135deg, #4b6cb7, #182848);
        color: white;
        padding: 1rem;
        border-radius: 10px 10px 0 0;
        text-align: center;
      }
      .chat-messages {
        flex: 1;
        overflow-y: auto;
        padding: 1rem;
      }
      .message {
        margin-bottom: 1rem;
        max-width: 80%;
        border-radius: 10px;
        padding: 0.75rem;
      }
      .user-message {
        background-color: #e6f7ff;
        align-self: flex-end;
        margin-left: auto;
      }
      .ai-message {
        background-color: #f0f2f5;
        align-self: flex-start;
      }
      .chat-input {
        display: flex;
        padding: 1rem;
        border-top: 1px solid #e6e6e6;
      }
      .listening-indicator {
        display: none;
        font-style: italic;
        color: #cc0000;
        text-align: center;
        margin-bottom: 0.5rem;
      }
      .mic-button {
        width: 60px;
        height: 60px;
        border-radius: 50%;
        background-color: #4b6cb7;
        color: white;
        border: none;
        margin: 0 auto;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.5rem;
        box-shadow: 0 3px 10px rgba(0, 0, 0, 0.2);
        transition: all 0.2s ease;
      }
      .mic-button:hover {
        background-color: #3a5a9b;
        transform: scale(1.05);
      }
      .mic-button.recording {
        background-color: #cc0000;
        animation: pulse 1.5s infinite;
      }
      @keyframes pulse {
        0% {
          transform: scale(1);
        }
        50% {
          transform: scale(1.1);
        }
        100% {
          transform: scale(1);
        }
      }
      .status-box {
        font-size: 0.85rem;
        padding: 0.5rem;
        margin: 0 1rem 1rem;
        border-radius: 5px;
        background-color: #f8f9fa;
        border-left: 3px solid #4b6cb7;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="chat-container">
        <div class="chat-header">
          <h3>MedAgent</h3>
          <p class="mb-0">AI Healthcare Assistant</p>
        </div>

        <div class="status-box" id="statusBox">
          Welcome to MedAgent! Click the microphone and speak to get started.
        </div>

        <div class="chat-messages" id="chatMessages">
          <div class="message ai-message">
            Hello! I'm your healthcare AI assistant. How can I help you today?
            Click the microphone button and speak when you're ready.
          </div>
        </div>

        <div class="listening-indicator" id="listeningIndicator">
          Listening...
        </div>

        <div class="chat-input">
          <button class="mic-button" id="micButton">
            <i class="bi bi-mic-fill">ðŸŽ¤</i>
          </button>
        </div>
      </div>
    </div>

    <script>
      let mediaRecorder;
      let audioChunks = [];
      const micButton = document.getElementById("micButton");
      const chatMessages = document.getElementById("chatMessages");
      const listeningIndicator = document.getElementById("listeningIndicator");
      const statusBox = document.getElementById("statusBox");

      // Initialize SpeechSynthesis
      const synth = window.speechSynthesis;

      // Check if browser supports necessary APIs
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        statusBox.textContent =
          "Your browser doesn't support audio recording. Please try Chrome or Firefox.";
        statusBox.style.borderLeft = "3px solid #cc0000";
        micButton.disabled = true;
      }

      // Add message to chat
      function addMessage(text, isUser = false) {
        const messageDiv = document.createElement("div");
        messageDiv.classList.add("message");
        messageDiv.classList.add(isUser ? "user-message" : "ai-message");
        messageDiv.textContent = text;
        chatMessages.appendChild(messageDiv);
        chatMessages.scrollTop = chatMessages.scrollHeight;
      }

      // Speak text using the Web Speech API
      function speak(text) {
        // Cancel any ongoing speech
        synth.cancel();

        // Create a new utterance
        const utterance = new SpeechSynthesisUtterance(text);

        // Optional: Configure voice properties
        utterance.rate = 1.0; // Speed
        utterance.pitch = 1.0; // Pitch

        // Get available voices and set a preferred one if available
        const voices = synth.getVoices();
        const preferredVoice = voices.find(
          (voice) =>
            voice.name.includes("Female") || voice.name.includes("Samantha")
        );

        if (preferredVoice) {
          utterance.voice = preferredVoice;
        }

        // Speak the text
        synth.speak(utterance);
      }

      // Start recording
      function startRecording() {
        navigator.mediaDevices
          .getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              channelCount: 1,
              sampleRate: 44100,
            },
          })
          .then((stream) => {
            // Update UI
            micButton.classList.add("recording");
            listeningIndicator.style.display = "block";
            statusBox.textContent = "Listening to your voice...";

            // Try different audio format options that are widely supported
            let options;
            try {
              // Try MP3 first (most compatible with Whisper)
              options = { mimeType: "audio/mpeg" };
              mediaRecorder = new MediaRecorder(stream, options);
            } catch (e1) {
              try {
                // Fall back to WebM
                options = { mimeType: "audio/webm" };
                mediaRecorder = new MediaRecorder(stream, options);
              } catch (e2) {
                // Last resort: use browser default
                mediaRecorder = new MediaRecorder(stream);
              }
            }

            audioChunks = [];

            // Handle data available event
            mediaRecorder.addEventListener("dataavailable", (event) => {
              audioChunks.push(event.data);
            });

            // Handle recording stop event
            mediaRecorder.addEventListener("stop", () => {
              // Update UI
              micButton.classList.remove("recording");
              listeningIndicator.style.display = "none";
              statusBox.textContent = "Processing your request...";

              // Create audio blob
              const audioType = mediaRecorder.mimeType || "audio/mp3";
              const audioBlob = new Blob(audioChunks, { type: audioType });
              const formData = new FormData();

              // Use file extension that matches the mime type
              const fileExtension = audioType.includes("webm") ? "webm" : "mp3";
              formData.append("audio", audioBlob, `recording.${fileExtension}`);

              fetch("/api/transcribe", {
                method: "POST",
                body: formData,
              })
                .then((response) => response.json())
                .then((data) => {
                  // Add user message
                  if (data.transcript) {
                    addMessage(data.transcript, true);

                    // Add AI response with speaking
                    if (data.response) {
                      addMessage(data.response, false);
                      speak(data.response);
                      statusBox.textContent = `Intent detected: ${data.intent}`;
                    }
                  } else {
                    statusBox.textContent =
                      "Sorry, I couldn't understand that. Please try again.";
                    statusBox.style.borderLeft = "3px solid #cc0000";
                  }
                })
                .catch((error) => {
                  console.error("Error:", error);
                  statusBox.textContent =
                    "There was an error processing your request. Please try again.";
                  statusBox.style.borderLeft = "3px solid #cc0000";
                });

              // Release microphone access
              stream.getTracks().forEach((track) => track.stop());
            });

            // Start recording
            mediaRecorder.start();

            // Record for longer (10 seconds) to get more audio
            setTimeout(() => {
              if (mediaRecorder.state !== "inactive") {
                mediaRecorder.stop();
              }
            }, 10000);
          })
          .catch((error) => {
            console.error("Error accessing microphone:", error);
            statusBox.textContent =
              "Unable to access your microphone. Please check permissions.";
            statusBox.style.borderLeft = "3px solid #cc0000";
          });
      }

      // Add click event to microphone button
      micButton.addEventListener("click", startRecording);

      // Load voices when the page loads (fix for some browsers)
      window.speechSynthesis.onvoiceschanged = function () {
        // This just ensures voices are loaded
        console.log(
          "Voices loaded:",
          window.speechSynthesis.getVoices().length
        );
      };
    </script>
  </body>
</html>
